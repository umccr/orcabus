AWSTemplateFormatVersion: 2010-09-09
Description: An S3 Batch Solution for Copying above 5GB S3 Object Size.
Metadata:
  License:
    Description: >-
      'Copyright 2017 Amazon.com, Inc. and its affiliates. All Rights Reserved.
      Licensed under the Amazon Software License (the "License"). You may not
      use this file except in compliance with the License. A copy of the License
      is located at

      http://aws.amazon.com/asl/

      or in the "license" file accompanying this file. This file is distributed
      on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
      express or implied. See the License for the specific language governing
      permissions and limitations under the License.'

  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: 'Destination Bucket and Prefix Parameters'
        Parameters:
          - BucketForCopyDestination
          - BucketForCopyDestinationPrefix
      - Label:
          default: 'S3 Batch Operations Manifest and Report Destination Parameters'
        Parameters:
          - BucketForManifestOrInventory
          - BucketForBatchOpsReport
      - Label:
          default: 'AWS SDK Performance, Chunksize and Retry Parameters'
        Parameters:
          - TransferMaximumConcurrency
          - SDKMaxPoolConnections
          - SDKMaxErrorRetries
          - MultipartChunkSize
      - Label:
          default: 'Copy Parameters'
        Parameters:
          - CopyMetadata
          - CopyTagging
          - StorageClass
          - RestrictSourceStorageClass

    ParameterLabels:
      BucketForCopyDestination:
        default: 'Destination Bucket'
      BucketForCopyDestinationPrefix:
        default: 'Destination Bucket Prefix'
      BucketForManifestOrInventory:
        default: 'Bucket where Manifest is Located'
      BucketForBatchOpsReport:
        default: 'Bucket where Batch Operations Report is Delivered'

Parameters:
  BucketForCopyDestination:
    Description: Please enter the Copy Destination S3 Bucket Name
    Type: String
    MinLength: '3'
    MaxLength: '63'
  BucketForCopyDestinationPrefix:
    Description: Please specify the Destination Bucket Prefix or Path without the starting or trailing "/", Leave Blank to Retain Original Path
    Type: String
    MinLength: '0'
    MaxLength: '1024'
  BucketForManifestOrInventory:
    Description: Please enter the S3 Bucket name where the CSV manifest or S3 Inventory is located
    Type: String
    MinLength: '3'
    MaxLength: '63'
  BucketForBatchOpsReport:
    Description: Please enter the S3 Bucket name you want Batch Operation Job Report Delivered to
    Type: String
    MinLength: '3'
    MaxLength: '63'
  TransferMaximumConcurrency:
    Description: The maximum number of concurrent requests SDK uses
    Type: String
    Default: 940
    MinLength: '1'
    MaxLength: '3'
    AllowedPattern: '[1-9][0-9]*'
    ConstraintDescription: Concurrent Requests Value must be a Valid Integer!
  SDKMaxPoolConnections:
    Description: The maximum number of connections SDK keeps in a connection pool
    Type: String
    Default: 940
    MinLength: '1'
    MaxLength: '3'
    AllowedPattern: '[1-9][0-9]*'
    ConstraintDescription: Concurrent Requests Value must be a Valid Integer!
  SDKMaxErrorRetries:
    Description: The number of SDK error retries
    Type: String
    Default: 100
    MinLength: '1'
    MaxLength: '3'
    AllowedPattern: '[1-9][0-9]*'
    ConstraintDescription: SDK Retries Value must be a Valid Integer!
  MultipartChunkSize:
    Description: Multipart Chunk size in bytes (MB*1024*1024) that the SDK uses for multipart transfers.
    Type: String
    Default: 16777216
    MinLength: '1'
    MaxLength: '10'
    AllowedPattern: '[1-9][0-9]*'
    ConstraintDescription: Multipart Chunk size Value must be a Valid Integer!
  CopyMetadata:
    AllowedValues:
      - Enable # Copies metadata
      - Disable # Do not copy metadata
    Description: Please Choose to enable or disable copying source object metadata to destination
    Type: String
  CopyTagging:
    AllowedValues:
      - Enable # Copies Tags
      - Disable # Do not copy Tags
    Description: Please Choose to enable or disable copying source object tags to destination
    Type: String
  StorageClass:
    AllowedValues:
      - STANDARD
      - STANDARD_IA
      - ONEZONE_IA
      - INTELLIGENT_TIERING
      - GLACIER_IR
      - GLACIER
      - DEEP_ARCHIVE
    Description: Please Choose your desired S3 Storage-Class for the Object Copy
    Default: STANDARD
    Type: String

Resources:
  S3BatchCopyLambdaFunctionIamRole:
    Type: 'AWS::IAM::Role'
    Properties:
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: S3BatchCopyLambdaFunctionIamRolePolicy0
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                  - 's3:GetObject'
                  - 's3:GetObjectAcl'
                  - 's3:GetObjectTagging'
                  - 's3:GetObjectVersion'
                  - 's3:GetObjectVersionAcl'
                  - 's3:GetObjectVersionTagging'
                  - 's3:ListBucket*'
                Resource: '*'
                Effect: Allow
              - Action:
                  - 's3:PutObject'
                  - 's3:PutObjectAcl'
                  - 's3:PutObjectTagging'
                  - 's3:PutObjectLegalHold'
                  - 's3:PutObjectRetention'
                  - 's3:GetBucketObjectLockConfiguration'
                  - 's3:ListBucket*'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:aws:s3:::${BucketForCopyDestination}
                  - !Sub arn:aws:s3:::${BucketForCopyDestination}/*
                Effect: Allow
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'

  S3BatchCopyLambdafunction:
    DependsOn:
      - S3BatchCopyLambdaFunctionIamRole
    Type: 'AWS::Lambda::Function'
    Properties:
      Environment:
        Variables:
          destination_bucket: !Ref BucketForCopyDestination
          destination_bucket_prefix: !Ref BucketForCopyDestinationPrefix
          max_attempts: !Ref SDKMaxErrorRetries
          max_concurrency: !Ref TransferMaximumConcurrency
          max_pool_connections: !Ref SDKMaxPoolConnections
          multipart_chunksize: !Ref MultipartChunkSize
          copy_metadata: !Ref CopyMetadata
          copy_tagging: !Ref CopyTagging
          copy_storage_class: !Ref StorageClass
      Runtime: python3.8
      Timeout: 900
      Description: An S3 Batch Solution for Copying above 5GB S3 Object Size.
      MemorySize: 600
      Handler: index.lambda_handler
      Role: !GetAtt
        - S3BatchCopyLambdaFunctionIamRole
        - Arn
      Code:
        ZipFile: |
          import boto3
          import os
          from urllib import parse
          from botocore.client import Config
          from botocore.exceptions import ClientError as S3ClientError
          from boto3.s3.transfer import TransferConfig
          import logging

          # Define Environmental Variables
          target_bucket = str(os.environ['destination_bucket'])
          my_max_pool_connections = int(os.environ['max_pool_connections'])
          my_max_concurrency = int(os.environ['max_concurrency'])
          my_multipart_chunksize = int(os.environ['multipart_chunksize'])
          my_max_attempts = int(os.environ['max_attempts'])
          metadata_copy = str(os.environ['copy_metadata'])
          tagging_copy = str(os.environ['copy_tagging'])
          obj_copy_storage_class = str(os.environ['copy_storage_class'])
          new_prefix = str(os.environ['destination_bucket_prefix'])

          # # Set up logging
          logger = logging.getLogger(__name__)
          logger.setLevel('INFO')

          # Enable Verbose logging for Troubleshooting
          # boto3.set_stream_logger("")

          # Set and Declare Configuration Parameters
          transfer_config = TransferConfig(max_concurrency=my_max_concurrency, multipart_chunksize=my_multipart_chunksize)
          config = Config(max_pool_connections=my_max_pool_connections, retries = {'max_attempts': my_max_attempts})
          myargs = {'ACL': 'bucket-owner-full-control', 'StorageClass': obj_copy_storage_class}


          # Instantiate S3Client
          s3Client = boto3.client('s3', config=config)



          def lambda_handler(event, context):
            # Parse job parameters from Amazon S3 batch operations
            jobId = event['job']['id']
            invocationId = event['invocationId']
            invocationSchemaVersion = event['invocationSchemaVersion']

            # Prepare results
            results = []

            # Parse Amazon S3 Key, Key Version, and Bucket ARN
            taskId = event['tasks'][0]['taskId']
            # use unquote_plus to handle various characters in S3 Key name
            s3Key = parse.unquote_plus(event['tasks'][0]['s3Key'], encoding='utf-8')
            s3VersionId = event['tasks'][0]['s3VersionId']
            s3BucketArn = event['tasks'][0]['s3BucketArn']
            s3Bucket = s3BucketArn.split(':')[-1]

            try:
              # Prepare result code and string
              resultCode = None
              resultString = None
              # Remove line feed or carriage return for compatibility with S3 Batch Result Message
              # Will use str.translate to strip '\n' and '\r'. Convert both char to ascii using ord()
              # where '\t' = 9, '\n' = 10 and '\r' = 13
              mycompat = {9: None, 10: None, 13: None}
              # Construct Copy Object
              copy_source = {'Bucket': s3Bucket, 'Key': s3Key}
              # If source key has VersionID, then construct request with VersionID
              if s3VersionId is not None:
                copy_source['VersionId'] = s3VersionId
                # Construct/Retrieve get source key metadata
                if metadata_copy == 'Enable':
                  get_metadata = s3Client.head_object(Bucket=s3Bucket, Key=s3Key, VersionId=s3VersionId)
                # Construct/Retrieve get source key tagging
                if tagging_copy == 'Enable':
                  get_obj_tag = s3Client.get_object_tagging(Bucket=s3Bucket, Key=s3Key, VersionId=s3VersionId)
              else:
                # Construct/Retrieve get source key metadata
                if metadata_copy == 'Enable':
                  get_metadata = s3Client.head_object(Bucket=s3Bucket, Key=s3Key)
                # Construct/Retrieve get source key tagging
                if tagging_copy == 'Enable':
                  get_obj_tag = s3Client.get_object_tagging(Bucket=s3Bucket, Key=s3Key)

              # Construct New Path
              # Construct New Key
              if new_prefix and len(new_prefix) > 0:
                newKey = "{0}/{1}".format(new_prefix, s3Key)
              else:
                newKey = s3Key

              newBucket = target_bucket

              # Toggle Metadata or Tagging Copy Based on Enviromental Variables
              # Construct Request Parameters with metadata and tagging from sourceKey
              # Create variables to append as metadata and tagging to destination object
              if metadata_copy == 'Enable':
                logger.info("Object Metadata Copy Enabled from Source to Destination")
                cache_control = get_metadata.get('CacheControl')
                content_disposition = get_metadata.get('ContentDisposition')
                content_encoding = get_metadata.get('ContentEncoding')
                content_language = get_metadata.get('ContentLanguage')
                metadata = get_metadata.get('Metadata')
                website_redirect_location = get_metadata.get('WebsiteRedirectLocation')
                expires = get_metadata.get('Expires')
                # Construct Request With Required and Available Arguments
                if cache_control:
                  myargs['CacheControl'] = cache_control
                if content_disposition:
                  myargs['ContentDisposition'] = content_disposition
                if content_encoding:
                  myargs['ContentEncoding'] = content_encoding
                if content_language:
                  myargs['ContentLanguage'] = content_language
                if metadata:
                  myargs['Metadata'] = metadata
                if website_redirect_location:
                  myargs['WebsiteRedirectLocation'] = website_redirect_location
                if expires:
                  myargs['Expires'] = expires
              else:
                logger.info("Object Metadata Copy Disabled")

              if tagging_copy == 'Enable':
                logger.info("Object Tagging Copy Enabled from Source to Destination")
                existing_tag_set = (get_obj_tag.get('TagSet'))
                # Convert the Output from get object tagging to be compatible with transfer s3.copy()
                tagging_to_s3 = "&".join([f"{parse.quote_plus(d['Key'])}={parse.quote_plus(d['Value'])}" for d in existing_tag_set])
                # Construct Request With Required and Available Arguments
                if existing_tag_set:
                  myargs['Tagging'] = tagging_to_s3
              else:
                logger.info("Object Tagging Copy Disabled")

              # Initiate the Actual Copy Operation and include transfer config option
              logger.info(f"starting copy of object {s3Key} with versionID {s3VersionId} between SOURCEBUCKET: {s3Bucket} and DESTINATIONBUCKET: {newBucket}")
              response = s3Client.copy(copy_source, newBucket, newKey, Config=transfer_config, ExtraArgs=myargs)
              # Confirm copy was successful
              logger.info("Successfully completed the copy process!")

              # Mark as succeeded
              resultCode = 'Succeeded'
              resultString = str(response)
            except S3ClientError as e:
              # log errors, some errors does not have a response, so handle them
              logger.error(f"Unable to complete requested operation, see Clienterror details below:")
              try:
                logger.error(e.response)
                errorCode = e.response.get('Error', {}).get('Code')
                errorMessage = e.response.get('Error', {}).get('Message')
                errorS3RequestID = e.response.get('ResponseMetadata', {}).get('RequestId')
                errorS3ExtendedRequestID = e.response.get('ResponseMetadata', {}).get('HostId')
                resultCode = 'PermanentFailure'
                resultString = '{}: {}: {}: {}'.format(errorCode, errorMessage, errorS3RequestID, errorS3ExtendedRequestID)
              except AttributeError:
                logger.error(e)
                resultCode = 'PermanentFailure'
                # Remove line feed or carriage return for compatibility with S3 Batch Result Message
                resultString = '{}'.format(str(e).translate(mycompat))
            except Exception as e:
              # log errors, some errors does not have a response, so handle them
              logger.error(f"Unable to complete requested operation, see Additional Client/Service error details below:")
              try:
                logger.error(e.response)
                errorCode = e.response.get('Error', {}).get('Code')
                errorMessage = e.response.get('Error', {}).get('Message')
                errorS3RequestID = e.response.get('ResponseMetadata', {}).get('RequestId')
                errorS3ExtendedRequestID = e.response.get('ResponseMetadata', {}).get('HostId')
                resultString = '{}: {}: {}: {}'.format(errorCode, errorMessage, errorS3RequestID, errorS3ExtendedRequestID)
              except AttributeError:
                logger.error(e)
                resultString = 'Exception: {}'.format(str(e).translate(mycompat))
              resultCode = 'PermanentFailure'

            finally:
              results.append({
              'taskId': taskId,
              'resultCode': resultCode,
              'resultString': resultString
              })

            return {
            'invocationSchemaVersion': invocationSchemaVersion,
            'treatMissingKeysAs': 'PermanentFailure',
            'invocationId': invocationId,
            'results': results
            }

  ############################ End Code ########################################
  S3BatchOperationsServiceIamRole:
    DependsOn:
      - S3BatchCopyLambdafunction
    Type: 'AWS::IAM::Role'
    Properties:
      Policies:
        - PolicyName: S3BatchOperationsServiceIamRolePolicy0
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:aws:s3:::${BucketForManifestOrInventory}
                  - !Sub arn:aws:s3:::${BucketForManifestOrInventory}/*
                Effect: Allow
              - Action:
                  - 's3:PutObject'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:aws:s3:::${BucketForBatchOpsReport}
                  - !Sub arn:aws:s3:::${BucketForBatchOpsReport}/*
                Effect: Allow
              - Action:
                  - 'lambda:InvokeFunction'
                Resource: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${S3BatchCopyLambdafunction}*'
                Effect: Allow
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: batchoperations.s3.amazonaws.com
            Action: 'sts:AssumeRole'

Outputs:
  DestinationS3Bucket:
    Description: The S3 Bucket where the solution will copy the objects to
    Value: !Sub ${BucketForCopyDestination}
  DestinationS3BucketPrefix:
    Description: The Prefix in the Destination S3 Bucket where the solution will copy the objects to
    Value: !Sub ${BucketForCopyDestinationPrefix}
  ManifestOrInventorySourceBucket:
    Description: The S3 Bucket where the CSV manifest or Inventory is located
    Value: !Sub ${BucketForManifestOrInventory}
  BatchOperationsReportDestination:
    Description: The S3 Bucket where S3 Batch Operation Job Report is delivered
    Value: !Sub ${BucketForBatchOpsReport}
  SDKRetries:
    Description: The number of times SDK will retry a failed request
    Value: !Sub ${SDKMaxErrorRetries}
  MaxConcurency:
    Description: The number of concurrent threads the SDK uses
    Value: !Sub ${TransferMaximumConcurrency}
  MaxPoolConnection:
    Description: The maximum number of HTTP/TCP connections pools SDK uses
    Value: !Sub ${SDKMaxPoolConnections}
  ObjectMultiPartChunkSize:
    Description: This is the chunk size that the SDK uses for multipart transfers of individual files
    Value: !Sub ${MultipartChunkSize}
